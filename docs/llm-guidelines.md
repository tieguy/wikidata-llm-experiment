# Guidelines for LLM-Assisted Wikidata Fact-Checking

> **Work in Progress**: This document was initially drafted with LLM assistance and requires further refinement and human fact-checking. Quotes and policy summaries should be verified against the original sources before being relied upon. The "Further Research Needed" section lists sources that could not be automatically retrieved and need manual review.

This document synthesizes Wikipedia and Wikidata community norms, policies, and concerns regarding the use of Large Language Models (LLMs) for contributing to Wikimedia projects. These guidelines inform how this research project approaches LLM-assisted fact-checking.

## Table of Contents

1. [Core Principles](#core-principles)
2. [Wikipedia Community Policies on LLMs](#wikipedia-community-policies-on-llms)
3. [Wikidata-Specific Requirements](#wikidata-specific-requirements)
4. [Known Risks and Failure Modes](#known-risks-and-failure-modes)
5. [Detection and Quality Control](#detection-and-quality-control)
6. [Recommended Practices for This Project](#recommended-practices-for-this-project)
7. [Further Research Needed](#further-research-needed)
8. [Sources](#sources)

---

## Core Principles

The Wikimedia community has articulated several foundational concerns about LLM use that shape all downstream policies:

### Verifiability is Non-Negotiable

> "All content must be verifiable. A fact or claim is 'verifiable' if a reliable source that supports it could be cited."
> — [Wikipedia:Verifiability](https://en.wikipedia.org/wiki/Wikipedia:Verifiability)

LLMs fundamentally challenge this principle because they:
- Generate plausible-sounding content without citations
- Hallucinate references that do not exist
- Cannot distinguish between what they "know" and what is verifiable from published sources

### Editor Responsibility Remains with Humans

> "As with all edits, an editor is fully responsible for their LLM-assisted edits."
> — [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models)

This means:
- LLM output requires human verification before use
- The human editor bears responsibility for policy compliance
- "The AI told me" is not a defense for policy violations

### Transparency is Expected

> "Every edit that incorporates LLM output should be marked as LLM-assisted by identifying the name and, if possible, version of the AI in the edit summary."
> — [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models)

False denial of LLM use when asked is likely to result in sanctions.

---

## Wikipedia Community Policies on LLMs

### Current Policy Status (as of 2025)

Wikipedia does not have a single "AI use policy." Instead, existing core content policies address AI-related concerns:

- **Verifiability** ([WP:V](https://en.wikipedia.org/wiki/Wikipedia:Verifiability)): All content must be attributable to reliable sources
- **No Original Research** ([WP:NOR](https://en.wikipedia.org/wiki/Wikipedia:No_original_research)): Wikipedia does not publish original thought
- **Neutral Point of View** ([WP:NPOV](https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view)): Content must be presented neutrally
- **Biographies of Living Persons** ([WP:BLP](https://en.wikipedia.org/wiki/Wikipedia:Biographies_of_living_persons)): Extra care required for content about living people

### The G15 Speedy Deletion Criterion

In August 2025, Wikipedia adopted [WP:G15](https://en.wikipedia.org/wiki/Wikipedia:Speedy_deletion#G15._LLM-generated_pages_without_human_review), which allows speedy deletion of:

> "Pages that could only plausibly have been generated by large language models and can be assumed not to have undergone reasonable human review."

This reflects community consensus that unreviewed LLM output is categorically unsuitable for Wikipedia.

### RFC Outcomes

A Request for Comments on LLM policy found no consensus on formal policy wording, with editors divided among:
1. Those favoring proposed disclosure requirements (largest group)
2. Those proposing complete bans on LLM use in article space
3. Those opposing mandatory disclosure due to detection difficulties

Key takeaway from discussions:
> "People want more restrictions on AI usage, we don't want to codify carveouts and exceptions."
> — [Wikipedia talk:Large language models](https://en.wikipedia.org/wiki/Wikipedia_talk:Large_language_models)

### Prohibited Uses

Per [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models):

- **Generating entire articles**: "LLMs should not be used to generate entire articles from scratch"
- **Bot-like editing**: "LLMs are not to be used for unapproved bot-like editing or anything approaching bot-like editing"
- **Writing talk page comments**: "Editors should not use LLMs to write comments generatively. Communication is at the root of Wikipedia's decision-making process and it's presumed editors can come up with their own ideas."

---

## Wikidata-Specific Requirements

### Sourcing Standards

From [Help:Sources](https://www.wikidata.org/wiki/Help:Sources) and [Wikidata:Data Import Guide](https://www.wikidata.org/wiki/Wikidata:Data_Import_Guide):

**Required reference properties:**
- `stated in` (P248): The publication or database where the information appears
- `reference URL` (P854): Direct link to the source
- `retrieved` (P813): Date the information was accessed

**For database imports:**
- Include the database identifier property
- Link to the specific record, not just the database homepage
- Enable programmatic verification against source material

**What doesn't count as a source:**
- `imported from Wikimedia project` (P143) alone is explicitly "not considered sourced"
- LLM "knowledge" without a citable reference

### Bot Policy

From [Wikidata:Bots](https://www.wikidata.org/wiki/Wikidata:Bots):

1. **Approval required**: Bots must be approved at [Wikidata:Requests for permissions/Bot](https://www.wikidata.org/wiki/Wikidata:Requests_for_permissions/Bot)
2. **Test runs**: Required before full approval
3. **Separate accounts**: Bot operators must use dedicated bot accounts
4. **Sourcing**: "Bots should add sources to any statement that is added unless it has been agreed the data is 'common knowledge'"
5. **Duplicate prevention**: "Bots should check that they are not adding duplicate statements"
6. **Rate limits**: 90 edits per minute maximum

### Mass Editing Policy

From [Wikidata:Requests for comment/Mass-editing policy](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/Mass-editing_policy):

> "Mass edits must be planned and executed with more care and sensitivity than other edits, because poor imports have a significant impact on existing data."

Concerns include:
- QuickStatements and OpenRefine enable unregulated mass editing
- "A lot of mass editing is done by non-bot accounts but is effectively unregulated bot work"
- Language barriers: "Editing at scale in languages the user cannot read and understand may be considered disruptive"

### Notability Criteria

From [Wikidata:Notability](https://www.wikidata.org/wiki/Wikidata:Notability), items are acceptable if they meet at least one criterion:

1. **Sitelinks**: Has a valid link to a Wikimedia project page
2. **Independent notability**: "Can be clearly identified by serious, publicly available references"
3. **Structural need**: Required to make statements on other notable items

### Statement Ranking

From [Help:Ranking](https://www.wikidata.org/wiki/Help:Ranking):

- **Normal rank**: Default; no judgment on accuracy
- **Preferred rank**: "The best among the correct statements" (e.g., current value)
- **Deprecated rank**: "Known to include errors" but preserved for historical reasons

> "References merely state where a data value comes from; ranks indicate what data value is considered the most correct."

---

## Known Risks and Failure Modes

### Hallucinated References

This is the most documented and serious risk:

> "A study that analyzed a total of 115 references provided by ChatGPT-3.5 documented that 47% of them were fabricated."
> — [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models)

> "In one analysis, researchers found an AI-written draft of a historical article included seven references – and five of those were fabricated."
> — [Wikipedia:Case against LLM-generated articles](https://en.wikipedia.org/wiki/Wikipedia:Case_against_LLM-generated_articles)

Hallucinated citations are worse than missing citations because they "give a false impression of credibility, undermining the reader's ability to trust Wikipedia."

### Fabricated Content

The most striking example documented by WikiProject AI Cleanup:

> "The article about the Ottoman fortress of Amberlisihar... had more than 2,000 words with detailed information about its name, construction, and sieges—but the fortress never existed. Everything in the article was fake, including the citations."
> — [Wikipedia:WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)

### Neutral Point of View Violations

LLMs systematically produce content that violates NPOV:

**Promotional language:**
> "AI tools often drift into advertising tone... phrases like 'rich cultural heritage,' 'breathtaking,' or 'stunning natural beauty.'"
> — [Wikipedia:Signs of AI writing](https://en.wikipedia.org/wiki/Wikipedia:Signs_of_AI_writing)

**Loss of specificity:**
> "LLMs tend to omit specific, unusual, nuanced facts and replace them with more generic, positive descriptions. The highly specific 'inventor of the first train-coupling device' might become 'a revolutionary titan of industry.'"
> — [Wikipedia:Signs of AI writing](https://en.wikipedia.org/wiki/Wikipedia:Signs_of_AI_writing)

**Editorial commentary:**
> "Phrases like 'it's important to note,' 'it is worth,' or 'no discussion would be complete without' introduce personal interpretation."
> — [Wikipedia:Signs of AI writing](https://en.wikipedia.org/wiki/Wikipedia:Signs_of_AI_writing)

### Biographies of Living Persons (BLP) Risks

LLM risks are "especially salient for biographies of living persons":

> "As LLMs often output accurate statements, and since their outputs are typically plausible-sounding and given with an air of confidence, users may have difficulty detecting problems. Even if 90% of the content is okay and 10% is false, that is a huge problem in an encyclopedia."
> — [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models)

BLP policy requires:
- Conservative writing with regard for privacy
- All contentious material must have inline citations to reliable sources
- Unsourced contentious material "must be removed immediately"

### Copyright Concerns

> "LLMs can generate material that violates copyright. Generated text may include verbatim snippets from non-free content or be a derivative work."
> — [Wikipedia:Large language models and copyright](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models_and_copyright)

> "The copyright status of LLMs trained on copyrighted material is not yet fully understood. Their output may not be compatible with the CC BY-SA license."
> — [Wikipedia:Large language models and copyright](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models_and_copyright)

### Volunteer Burnout

> "What used to be a rewarding hobby of knowledge-sharing is at risk of becoming an 'increasingly investigative and adversarial process.' The community fears volunteer burnout as editors struggle to keep up with a flood of machine-produced material."
> — [AI & Society: An endangered species](https://link.springer.com/article/10.1007/s00146-025-02199-9)

As of October 2025, nearly 3,000 articles have been tagged for suspected AI content on English Wikipedia alone.

---

## Detection and Quality Control

### Signs of AI-Generated Content

From [Wikipedia:Signs of AI writing](https://en.wikipedia.org/wiki/Wikipedia:Signs_of_AI_writing):

**Obvious artifacts:**
- "As an AI language model, I..."
- "As of my last knowledge update..."
- "I hope this helps," "Certainly!", "let me know"
- Knowledge cutoff disclaimers

**Formatting patterns:**
- Title case for all section headings
- "Bullet points with bold titles" style
- Unnecessary "Conclusion" sections arguing significance

**Language patterns:**
- Excessive em dashes
- Overuse of "moreover," "delve," "tapestry"
- Promotional superlatives
- Weasel words without attribution

**Reference problems:**
- Broken links and 404 errors
- Fake DOIs or invalid ISBN checksums
- Academic-sounding but non-existent publications

### Limitations of Automated Detection

> "Editors should not solely rely on artificial intelligence content detection tools (such as GPTZero). While they perform better than random chance, these tools have non-trivial error rates."
> — [Wikipedia:WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)

> "Deleting or tagging content purely because it was flagged by an automatic AI detector is not acceptable."
> — [Wikipedia:WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)

### Wikidata Quality Tools

- **Property constraints**: Rules that flag likely errors (e.g., head of government should be human)
- **Constraint violation reports**: [Wikidata:Database reports/Constraint violations](https://www.wikidata.org/wiki/Wikidata:Database_reports/Constraint_violations)
- **Recoin gadget**: Monitors relative completeness
- **Mix'N'Match**: Validates and corrects external links

Community survey on constraint violations:
> "When asked how often the data needs to be corrected when a constraint violation appears, most active editors (63%) said 'often.'"
> — [A Study of the Quality of Wikidata](https://arxiv.org/pdf/2107.00156)

---

## Recommended Practices for This Project

Based on community norms and concerns, this project adopts the following practices:

### 1. LLMs as Research Assistants, Not Content Generators

LLMs in this project are used to:
- Identify what claims need verification
- Suggest where to look for sources
- Help interpret source content
- Draft structured output for human review

LLMs are NOT used to:
- Generate claims to add to Wikidata
- Fabricate or guess reference information
- Make unsupervised edits

### 2. Source-First Methodology

Every claim must trace to an actual, verifiable source:
- Primary sources (official websites, government records) preferred
- Secondary sources acceptable with appropriate skepticism
- LLM "knowledge" is never treated as a source

### 3. Mandatory Human Review

All fact-checking output is logged for human review before any edit action. The human reviewer verifies:
- The source actually exists and says what we claim
- The claim is appropriate for the item
- References are properly formatted
- No BLP or copyright concerns

### 4. Transparency in Logging

All sessions are logged in machine-readable format including:
- Which LLM was used and for what purpose
- What sources were consulted
- Confidence levels and reasoning
- Human review decisions

### 5. Test Environment Only

All pywikibot operations target `test.wikidata.org` exclusively. This is enforced by configuration but should be verified before any edit operation.

### 6. Respect for Rate Limits and Norms

Even on test Wikidata:
- Respect maxlag and API etiquette
- No mass editing without consideration of impact
- Quality over quantity

### 7. Awareness of Failure Modes

Active monitoring for:
- Hallucinated references (verify every URL)
- Promotional language in descriptions
- False precision in dates and values
- Constraint violations before adding claims

---

## Further Research Needed

The following resources could not be automatically fetched during the initial research phase and require manual review to verify, correct, or expand the information in this document:

### Wikipedia Policy Pages (403 errors)
- [Wikipedia:Case against LLM-generated articles](https://en.wikipedia.org/wiki/Wikipedia:Case_against_LLM-generated_articles) - Comprehensive arguments against LLM-generated content
- [Wikidata:Bots](https://www.wikidata.org/wiki/Wikidata:Bots) - Full bot policy details
- [Help:Sources](https://www.wikidata.org/wiki/Help:Sources) - Complete sourcing guidelines

### Potentially Relevant RFCs
- [Wikidata:Requests for comment/Sourcing requirements for bots](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/Sourcing_requirements_for_bots)
- [Wikidata:Requests for comment/References and sources](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/References_and_sources)
- [Wikidata:Requests for comment/Improve bot policy for data import](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/Improve_bot_policy_for_data_import_and_data_modification)

### Academic Research
- [Death by AI: Will large language models diminish Wikipedia?](https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24975) (Wagner, 2025)
- [Wikipedia in the Era of LLMs: Evolution and Risks](https://arxiv.org/html/2503.02879v1)
- [Machines in the Margins: A Systematic Review of Automated Content Generation for Wikipedia](https://arxiv.org/html/2509.22443)
- [An endangered species: how LLMs threaten Wikipedia's sustainability](https://link.springer.com/article/10.1007/s00146-025-02199-9)
- [Research:How LLMs impact knowledge production processes](https://meta.wikimedia.org/wiki/Research:How_LLMs_impact_knowledge_production_processes)

### Community Discussions
- [Wikipedia talk:Large language models](https://en.wikipedia.org/wiki/Wikipedia_talk:Large_language_models) and archives
- [Wikipedia talk:Writing articles with large language models](https://en.wikipedia.org/wiki/Wikipedia_talk:Writing_articles_with_large_language_models)

---

## Sources

### Wikipedia Policies and Guidelines
- [Wikipedia:Large language models](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models)
- [Wikipedia:Verifiability](https://en.wikipedia.org/wiki/Wikipedia:Verifiability)
- [Wikipedia:Reliable sources](https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources)
- [Wikipedia:Neutral point of view](https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view)
- [Wikipedia:Biographies of living persons](https://en.wikipedia.org/wiki/Wikipedia:Biographies_of_living_persons)
- [Wikipedia:Signs of AI writing](https://en.wikipedia.org/wiki/Wikipedia:Signs_of_AI_writing)
- [Wikipedia:WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)
- [Wikipedia:WikiProject AI Cleanup/Policies](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup/Policies)
- [Wikipedia:Large language models and copyright](https://en.wikipedia.org/wiki/Wikipedia:Large_language_models_and_copyright)
- [Wikipedia:Artificial intelligence in Wikimedia projects](https://en.wikipedia.org/wiki/Artificial_intelligence_in_Wikimedia_projects)
- [Wikipedia:Manual of Style/Words to watch](https://en.wikipedia.org/wiki/Wikipedia:Avoid_weasel_words)

### Wikidata Policies and Guidelines
- [Wikidata:Bots](https://www.wikidata.org/wiki/Wikidata:Bots)
- [Wikidata:Notability](https://www.wikidata.org/wiki/Wikidata:Notability)
- [Wikidata:Data Import Guide](https://www.wikidata.org/wiki/Wikidata:Data_Import_Guide)
- [Help:Sources](https://www.wikidata.org/wiki/Help:Sources)
- [Help:Ranking](https://www.wikidata.org/wiki/Help:Ranking)
- [Help:Property constraints portal](https://www.wikidata.org/wiki/Help:Property_constraints_portal)
- [Wikidata:Requests for comment/Mass-editing policy](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/Mass-editing_policy)
- [Wikidata:Requests for comment/Bot policy](https://www.wikidata.org/wiki/Wikidata:Requests_for_comment/Bot_policy)
- [Wikidata:Database reports/Constraint violations](https://www.wikidata.org/wiki/Wikidata:Database_reports/Constraint_violations)
- [Wikidata:WikiProject Data Quality](https://www.wikidata.org/wiki/Wikidata:WikiProject_Data_Quality)
- [Wikidata:Vandalism](https://www.wikidata.org/wiki/Wikidata:Vandalism)

### News and Analysis
- [Wikipedia editors publish new guide to help readers detect entries written by AI](https://www.npr.org/2025/09/04/nx-s1-5519267/wikipedia-editors-publish-new-guide-to-help-readers-detect-entries-written-by-ai) (NPR, 2025)
- [Here's how to spot AI writing, according to Wikipedia editors](https://the-decoder.com/heres-how-to-spot-ai-writing-according-to-wikipedia-editors/) (The Decoder, 2025)
- [The 3 building blocks of trustworthy information: Lessons from Wikipedia](https://wikimediafoundation.org/news/2025/10/02/the-3-building-blocks-of-trustworthy-information-lessons-from-wikipedia/) (Wikimedia Foundation, 2025)

### Academic Research
- [A Study of the Quality of Wikidata](https://arxiv.org/pdf/2107.00156) (Piscopo & Simperl)
- [An endangered species: how LLMs threaten Wikipedia's sustainability](https://link.springer.com/article/10.1007/s00146-025-02199-9) (AI & Society, 2025)
- [Building Automated Vandalism Detection Tools for Wikidata](https://wikiworkshop.org/2017/papers/p1647-sarabadani.pdf) (Sarabadani, 2017)

---

*Last updated: 2025-01-19*
*This document is part of the wikidata-llm-experiment research project.*
