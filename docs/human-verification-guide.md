# Human Verification Guide for SIFT Methodology Testing

## Purpose

This guide explains how to verify proposed claims generated by the SIFT methodology testing skill. Your verification is the ground truth for measuring accuracy.

## Verification Workflow

### 1. Review the Proposed Claim

Each log file in `logs/wikidata-methodology-testing/` contains a proposed claim. Review:

- **Entity:** Is this the correct entity?
- **Property:** Is this an appropriate property for this entity?
- **Proposed Value:** Does this value seem plausible?
- **Confidence:** Does the stated confidence match your assessment?

### 2. Verify Sources

For each source in `sources_consulted`:

1. **Open the URL** - Does it load? Is it the correct page?
2. **Find the claim** - Does the source actually contain information about this property?
3. **Check accuracy** - Does the source support the proposed value?

**Common issues:**
- URL returns 404 (hallucinated source)
- URL exists but doesn't mention the entity (wrong source)
- Source mentions entity but not this property (misread source)
- Source has different value than proposed (incorrect value)

### 3. Verify SIFT Analysis

Review the `sift_verification` section:

- **Stop:** Did the analysis question appropriate assumptions?
- **Investigate:** Is the source assessment accurate?
- **Find:** Were cross-references actually found and consulted?
- **Trace:** Was a primary source identified (if applicable)?

### 4. Fill in Human Verification

Update the `human_verification` section of the YAML file:

```yaml
human_verification:
  reviewed_by: "[your name]"
  review_date: "[YYYY-MM-DD]"
  sift_correct: [true|false]  # Was the SIFT analysis correct?
  proposed_value_correct: [true|false]  # Is the proposed value correct?
  actual_value: "[value]"  # If different from proposed, what's correct?
  failure_mode: "[mode]"  # If incorrect, why? (see taxonomy below)
  notes: "[any additional notes]"
```

## Failure Mode Taxonomy

When `sift_correct: false` or `proposed_value_correct: false`, classify the failure:

| Failure Mode | Description | Example |
|--------------|-------------|---------|
| `hallucinated_source` | Source URL doesn't exist or returns 404 | LLM cited a URL that was never real |
| `wrong_source` | Source exists but is about different entity/topic | Cited a page about a different person with same name |
| `misread_source` | Source exists but doesn't support the claim | Source mentions entity but not this property |
| `incorrect_value` | Source supports a claim but with different value | Source says 1952, LLM proposed 1953 |
| `wrong_property` | Proposed wrong property for this type of information | Used P569 (birth) when should be P571 (inception) |
| `insufficient_precision` | Value correct but precision wrong | Source says "1950s", LLM claimed specific date |
| `outdated_source` | Source was accurate but information has changed | Company HQ moved since source was published |
| `conflicting_sources` | Sources disagree and LLM chose wrong one | Two sources give different dates |
| `primary_source_missing` | Claimed primary source but it wasn't traced | Said "per official records" but cited Wikipedia |
| `other` | Doesn't fit above categories | Document in notes |

## Verification Standards

### What counts as "SIFT correct"?

SIFT is correct if:
- Sources cited actually exist and are accessible
- Sources actually contain information about the claimed property
- Source assessment (reliability, type) is reasonable
- Cross-referencing was attempted where appropriate
- Primary sources were traced where available

SIFT is incorrect if:
- Any source is hallucinated
- Source assessment is significantly wrong
- Claims about cross-referencing are false
- Primary source claim is unfounded

### What counts as "value correct"?

Value is correct if:
- The proposed value matches what reliable sources say
- Precision is appropriate (don't claim day precision if source only gives year)
- Value type is correct (item vs string vs date)

Value is incorrect if:
- Sources give a different value
- Value is an extrapolation beyond what sources support
- Precision is overstated

## Tips for Efficient Verification

1. **Batch by entity type** - Verify all humans, then all orgs, then all works
2. **Keep source tabs open** - Often multiple claims cite same sources
3. **Use Wikidata as cross-reference** - Check what Wikidata already has for the entity
4. **Note patterns** - If you see the same failure mode repeatedly, note it
5. **Time yourself** - Track how long verification takes for later analysis

## Questions?

If you encounter a case that doesn't fit this guide, document it thoroughly in the notes field and flag for discussion.
